{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ptr75zrzBjSq"
      },
      "outputs": [],
      "source": [
        "import re, time, hashlib, requests, tldextract\n",
        "from urllib.parse import urljoin, urldefrag\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ViT7Qf6jMWgc"
      },
      "outputs": [],
      "source": [
        "#Crawler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7gPMLPxzMPcT"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urldefrag\n",
        "import time\n",
        "\n",
        "HEADERS = {\"User-Agent\": \"MySimpleCrawler/1.0\"}\n",
        "SKIP_EXT = (\".pdf\", \".jpg\", \".jpeg\", \".png\", \".gif\", \".zip\", \".mp4\", \".mp3\")\n",
        "\n",
        "def normalize_url(url):\n",
        "    url = urldefrag(url)[0]\n",
        "    return url.strip()\n",
        "\n",
        "def get_links(base_url, html):\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    return [urljoin(base_url, a[\"href\"]) for a in soup.find_all(\"a\", href=True)]\n",
        "\n",
        "def clean_text(html):\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
        "        tag.decompose()\n",
        "    return soup.get_text(\" \", strip=True)\n",
        "\n",
        "def crawl_site(seed_url, max_pages=3, delay=1):\n",
        "    seed_url = normalize_url(seed_url)\n",
        "    visited = set()\n",
        "    to_visit = [seed_url]\n",
        "    pages = []\n",
        "\n",
        "    while to_visit and len(pages) < max_pages:\n",
        "        url = to_visit.pop(0)\n",
        "        if url in visited or url.lower().endswith(SKIP_EXT):\n",
        "            continue\n",
        "        try:\n",
        "            print(f\"Fetching: {url}\")\n",
        "            resp = requests.get(url, headers=HEADERS, timeout=10)\n",
        "            if \"text/html\" not in resp.headers.get(\"Content-Type\", \"\"):\n",
        "                continue\n",
        "            text = clean_text(resp.text)\n",
        "            pages.append({\"url\": url, \"text\": text})\n",
        "            visited.add(url)\n",
        "            to_visit.extend([normalize_url(link) for link in get_links(url, resp.text) if link not in visited])\n",
        "            time.sleep(delay)\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching {url}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return pages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LpPJUfyKMY0o"
      },
      "outputs": [],
      "source": [
        "#chunker\n",
        "\n",
        "def chunk_text(text, max_words=220, overlap=40):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    while i < len(words):\n",
        "        chunks.append(\" \".join(words[i:i+max_words]))\n",
        "        if i + max_words >= len(words):\n",
        "            break\n",
        "        i += max_words - overlap\n",
        "    return chunks\n",
        "\n",
        "def pages_to_chunks(pages, max_words=220, overlap=40):\n",
        "    docs = []\n",
        "    for p in pages:\n",
        "        chs = chunk_text(p[\"text\"], max_words=max_words, overlap=overlap)\n",
        "        for idx, ch in enumerate(chs):\n",
        "            docs.append({\n",
        "                \"url\": p[\"url\"],\n",
        "                \"content\": ch,\n",
        "                \"chunk_id\": f'{p[\"url\"]}#chunk={idx}'\n",
        "            })\n",
        "    return docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1UhZuAGXMfIi"
      },
      "outputs": [],
      "source": [
        "#FAISS VectorStore (save as my_vectorstore.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qsa4UyRiNUa2"
      },
      "outputs": [],
      "source": [
        "#4. retriver\n",
        "from my_vectorstore import SimpleVectorStore\n",
        "\n",
        "class Retriever:\n",
        "    def __init__(self, index_dir: str):\n",
        "        self.store = SimpleVectorStore(index_dir)\n",
        "        self.store.load()\n",
        "\n",
        "    def get_context(self, question: str, top_k=5):\n",
        "        return self.store.search(question, top_k=top_k)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RlhCAlDFTVDn"
      },
      "outputs": [],
      "source": [
        "#gpt 4 all q and a\n",
        "\n",
        "from gpt4all import GPT4All\n",
        "\n",
        "# Load local GPT4All model (small free version)\n",
        "#model = GPT4All(\"ggml-gpt4all-j-v1.3-groovy\")  # runs on CPU in Colab\n",
        "\n",
        "#from gpt4all import GPT4All\n",
        "model = GPT4All(\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\") # downloads / loads a 4.66GB LLM\n",
        "# with model.chat_session():\n",
        "#     print(model.generate(\"How can I run LLMs efficiently on my laptop?\", max_tokens=1024))\n",
        "\n",
        "\n",
        "SYSTEM_PROMPT = \"Answer the question only using the provided context. Cite sources. If not found, say 'Not found on this site.'\"\n",
        "\n",
        "\n",
        "def format_context(chunks):\n",
        "    formatted = []\n",
        "    for i, c in enumerate(chunks, 1):\n",
        "        formatted.append(f\"[{i}] {c['content']}\\n(Source: {c['url']})\")\n",
        "    return \"\\n\\n\".join(formatted)\n",
        "\n",
        "def answer_with_gpt4all(question, chunks):\n",
        "    context = format_context(chunks)\n",
        "    prompt = f\"{SYSTEM_PROMPT}\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\"\n",
        "    response = model.generate(prompt, max_tokens=400)\n",
        "\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVTz2Pc_VAlD",
        "outputId": "f8b72eca-bdab-46d6-ad4c-2aafae72601c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching: https://en.wikipedia.org/wiki/Python_(programming_language\n",
            "Answer:\n",
            "  programming language?\n",
            "Answer:\n",
            "According to the provided context from Wikipedia [1], Python (programming language) is described as:\n",
            "\n",
            "\"Python is a high-level, interpreted programming language that is widely used for various purposes such as web development, scientific computing, data analysis, artificial intelligence, and more.\"\n",
            "\n",
            "Source: https://en.wikipedia.org/wiki/Python_(programming_language)\n",
            "\n",
            "Note: The provided context does not contain any information about the page being deleted or created. [2] and [3] are irrelevant to this question.\n",
            "\n",
            "References:\n",
            "[1] Wikipedia (2020). Python (programming language). Retrieved from <https://en.wikipedia.org/wiki/Python_(programming_language)>.\n",
            "Not found on this site: The provided context does not contain any information about the page being deleted or created. [2] and [3] are irrelevant to this question. Therefore, there is no reference for this part of the answer.\n"
          ]
        }
      ],
      "source": [
        "# 1️⃣ Crawl website\n",
        "pages = crawl_site(\"https://en.wikipedia.org/wiki/Python_(programming_language\", max_pages=1)\n",
        "\n",
        "# 2️⃣ Chunk pages\n",
        "chunks = pages_to_chunks(pages, max_words=100, overlap=20)\n",
        "\n",
        "# 3️⃣ Build FAISS vector store\n",
        "store = SimpleVectorStore(\"vector_index\")\n",
        "store.build(chunks)\n",
        "store.save()\n",
        "\n",
        "# 4️⃣ Use Retriever\n",
        "retriever = Retriever(\"vector_index\")\n",
        "relevant_chunks = retriever.get_context(\"what is python\", top_k=3)\n",
        "\n",
        "# 5️⃣ Ask GPT4All locally\n",
        "answer = answer_with_gpt4all(\"what is python\", relevant_chunks)\n",
        "print(\"Answer:\\n\", answer)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
